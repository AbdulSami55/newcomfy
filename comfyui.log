** ComfyUI startup time: 2024-02-09 17:26:41.402466
[2024-02-09 17:26] ** Platform: Windows
[2024-02-09 17:26] ** Python version: 3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:01:18) [MSC v.1916 64 bit (AMD64)]
[2024-02-09 17:26] ** Python executable: C:\Users\AbdulSami\anaconda3\envs\env\python.exe
[2024-02-09 17:26] ** Log path: D:\CorTechSols\ComfyUI\comfyui.log
[2024-02-09 17:26] 
Prestartup times for custom nodes:
[2024-02-09 17:26]    0.0 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-02-09 17:26] 
[2024-02-09 17:26] Total VRAM 4096 MB, total RAM 32409 MB
[2024-02-09 17:26] Trying to enable lowvram mode because your GPU seems to have 4GB or less. If you don't want this use: --normalvram
[2024-02-09 17:26] Set vram state to: LOW_VRAM
[2024-02-09 17:26] Device: cuda:0 NVIDIA GeForce RTX 2050 : cudaMallocAsync
[2024-02-09 17:26] VAE dtype: torch.bfloat16
[2024-02-09 17:26] Using pytorch cross attention
[2024-02-09 17:26] aaa None
[2024-02-09 17:26] ### Loading: ComfyUI-Impact-Pack (V4.70)
[2024-02-09 17:26] ### Loading: ComfyUI-Impact-Pack (Subpack: V0.4)
[2024-02-09 17:26] ### Loading: ComfyUI-Manager (V2.6)
[2024-02-09 17:26] [Impact Pack] Wildcards loading done.
[2024-02-09 17:26] 

## [WARN] ComfyUI-Manager: Your ComfyUI version (1923)[2024-01-20] is too old. Please update to the latest version. ##

[2024-02-09 17:26] 
[2024-02-09 17:26] ### ComfyUI Revision: 1923 [ef5a28b5] | Released on '2024-01-20'
[36;20m[comfyui_controlnet_aux] | INFO -> Using ckpts path: D:\CorTechSols\ComfyUI\custom_nodes\comfyui_controlnet_aux\ckpts[0m
[2024-02-09 17:26] [36;20m[comfyui_controlnet_aux] | INFO -> Using symlinks: False[0m
[2024-02-09 17:26] [36;20m[comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider'][0m
[2024-02-09 17:26] D:\CorTechSols\ComfyUI\custom_nodes\comfyui_controlnet_aux\node_wrappers\dwpose.py:26: UserWarning: DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly
  warnings.warn("DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly")
[2024-02-09 17:26] 
Import times for custom nodes:
[2024-02-09 17:26]    0.0 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-Video-Matting
[2024-02-09 17:26]    0.0 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-Advanced-ControlNet
[2024-02-09 17:26]    0.0 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-AnimateDiff-Evolved
[2024-02-09 17:26]    0.4 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-VideoHelperSuite
[2024-02-09 17:26]    0.6 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-02-09 17:26]    0.7 seconds: D:\CorTechSols\ComfyUI\custom_nodes\comfyui_controlnet_aux
[2024-02-09 17:26]    2.7 seconds: D:\CorTechSols\ComfyUI\custom_nodes\ComfyUI-Impact-Pack
[2024-02-09 17:26] 
[2024-02-09 17:26] Starting server
[2024-02-09 17:26] 
[2024-02-09 17:26] To see the GUI go to: http://127.0.0.1:8188
[2024-02-09 17:26] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[2024-02-09 17:26] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[2024-02-09 17:26] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[2024-02-09 17:26] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
[2024-02-09 17:26] None
[2024-02-09 17:26] /get-output-image
[2024-02-09 17:28] None
[2024-02-09 17:28] /get-output-image
[2024-02-09 17:28] None
[2024-02-09 17:28] /view
[2024-02-09 17:29] None
[2024-02-09 17:29] /get-output-image
[2024-02-09 17:29] None
[2024-02-09 17:29] /view
[2024-02-09 17:29] None
[2024-02-09 17:29] /get-output-image
[2024-02-09 17:29] None
[2024-02-09 17:29] /view
[2024-02-09 17:30] None
[2024-02-09 17:30] /get-output-image
[2024-02-09 17:30] None
[2024-02-09 17:30] /view
[2024-02-09 17:30] None
[2024-02-09 17:30] /get-output-image
[2024-02-09 17:30] None
[2024-02-09 17:30] /view
[2024-02-09 17:32] None
[2024-02-09 17:32] /get-output-image
[2024-02-09 17:32] None
[2024-02-09 17:32] /view
[2024-02-09 17:32] None
[2024-02-09 17:32] /get-output-image
[2024-02-09 17:32] None
[2024-02-09 17:32] /view
[2024-02-09 17:35] None
[2024-02-09 17:35] /view
[2024-02-09 19:07] None
[2024-02-09 19:07] /get-output-image
[2024-02-09 19:07] None
[2024-02-09 19:07] /view
[2024-02-09 19:08] None
[2024-02-09 19:08] /get-output-image
[2024-02-09 19:08] None
[2024-02-09 19:08] /view
[2024-02-09 19:09] None
[2024-02-09 19:09] /view
[2024-02-09 19:09] None
[2024-02-09 19:09] /get-output-image
[2024-02-09 19:09] None
[2024-02-09 19:09] /get-output-image
[2024-02-09 19:09] None
[2024-02-09 19:09] /get-output-image
[2024-02-09 19:09] None
[2024-02-09 19:09] /view
[2024-02-09 19:25] None
[2024-02-09 19:25] /upload/image
[2024-02-09 19:25] None
[2024-02-09 19:25] /upload/image
[2024-02-09 19:26] None
[2024-02-09 19:26] /upload/image
[2024-02-09 19:31] None
[2024-02-09 19:31] /get-output-image
[2024-02-09 19:31] None
[2024-02-09 19:31] /view
[2024-02-09 19:31] None
[2024-02-09 19:31] /get-output-image
[2024-02-09 19:31] None
[2024-02-09 19:31] /view
[2024-02-09 19:31] None
[2024-02-09 19:31] /get-output-image
[2024-02-09 19:31] None
[2024-02-09 19:31] /view
[2024-02-09 19:31] None
[2024-02-09 19:31] /get-output-image
[2024-02-09 19:31] None
[2024-02-09 19:31] /view
[2024-02-09 19:31] None
[2024-02-09 19:31] /upload/image
[2024-02-09 19:31] None
[2024-02-09 19:31] /get-output-image
[2024-02-09 19:31] None
[2024-02-09 19:31] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:35] None
[2024-02-09 19:35] /get-output-image
[2024-02-09 19:35] None
[2024-02-09 19:35] /view
[2024-02-09 19:36] None
[2024-02-09 19:36] /upload/image
[2024-02-09 19:36] got prompt
[2024-02-09 19:36] [WARN] ComfyUI-Impact-Pack: Error on prompt - several features will not work.
'inputs'
[2024-02-09 19:36] {'prompt': {'prompt': {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'red hair', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00002_ (4).png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}, 'extra_data': {'extra_pnginfo': {'workflow': {'last_node_id': 16, 'last_link_id': 15, 'nodes': [{'id': 5, 'type': 'EmptyLatentImage', 'pos': [473, 609], 'size': {'0': 315, '1': 106}, 'flags': {}, 'order': 0, 'mode': 0, 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [], 'slot_index': 0}], 'properties': {'Node name for S&R': 'EmptyLatentImage'}, 'widgets_values': [512, 512, 1]}, {'id': 9, 'type': 'SaveImage', 'pos': [1451, 189], 'size': {'0': 210, '1': 270}, 'flags': {}, 'order': 8, 'mode': 0, 'inputs': [{'name': 'images', 'type': 'IMAGE', 'link': 12}], 'properties': {}, 'widgets_values': ['ComfyUI']}, {'id': 14, 'type': 'VAEDecode', 'pos': [1210, 188], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 7, 'mode': 0, 'inputs': [{'name': 'samples', 'type': 'LATENT', 'link': 10}, {'name': 'vae', 'type': 'VAE', 'link': 11}], 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [12], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEDecode'}}, {'id': 10, 'type': 'LoadImage', 'pos': [867, 587], 'size': [315, 313.9999694824219], 'flags': {}, 'order': 1, 'mode': 0, 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [13], 'shape': 3, 'slot_index': 0}, {'name': 'MASK', 'type': 'MASK', 'links': 'null', 'shape': 3}], 'properties': {'Node name for S&R': 'LoadImage'}, 'widgets_values': ['3 idiots.jpg', 'image']}, {'id': 4, 'type': 'CheckpointLoaderSimple', 'pos': [26, 474], 'size': {'0': 315, '1': 98}, 'flags': {}, 'order': 2, 'mode': 0, 'outputs': [{'name': 'MODEL', 'type': 'MODEL', 'links': [1], 'slot_index': 0}, {'name': 'CLIP', 'type': 'CLIP', 'links': [3, 5], 'slot_index': 1}, {'name': 'VAE', 'type': 'VAE', 'links': [11, 14], 'slot_index': 2}], 'properties': {'Node name for S&R': 'CheckpointLoaderSimple'}, 'widgets_values': ['animagineXLV3_v30.safetensors']}, {'id': 16, 'type': 'VAEEncode', 'pos': [1316, 665], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 5, 'mode': 0, 'inputs': [{'name': 'pixels', 'type': 'IMAGE', 'link': 13}, {'name': 'vae', 'type': 'VAE', 'link': 14}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [15], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEEncode'}}, {'id': 3, 'type': 'KSampler', 'pos': [863, 186], 'size': {'0': 315, '1': 262}, 'flags': {}, 'order': 6, 'mode': 0, 'inputs': [{'name': 'model', 'type': 'MODEL', 'link': 1}, {'name': 'positive', 'type': 'CONDITIONING', 'link': 4}, {'name': 'negative', 'type': 'CONDITIONING', 'link': 6}, {'name': 'latent_image', 'type': 'LATENT', 'link': 15}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [10], 'slot_index': 0}], 'properties': {'Node name for S&R': 'KSampler'}, 'widgets_values': [618284639060744, 'randomize', 20, 8, 'euler', 'normal', 1]}, {'id': 6, 'type': 'CLIPTextEncode', 'pos': [415, 186], 'size': {'0': 422.84503173828125, '1': 164.31304931640625}, 'flags': {}, 'order': 3, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 3}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [4], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['change the background to blue and keep the front people the same']}, {'id': 7, 'type': 'CLIPTextEncode', 'pos': [413, 389], 'size': {'0': 425.27801513671875, '1': 180.6060791015625}, 'flags': {}, 'order': 4, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 5}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [6], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['text, watermark,person']}], 'links': [[1, 4, 0, 3, 0, 'MODEL'], [3, 4, 1, 6, 0, 'CLIP'], [4, 6, 0, 3, 1, 'CONDITIONING'], [5, 4, 1, 7, 0, 'CLIP'], [6, 7, 0, 3, 2, 'CONDITIONING'], [10, 3, 0, 14, 0, 'LATENT'], [11, 4, 2, 14, 1, 'VAE'], [12, 14, 0, 9, 0, 'IMAGE'], [13, 10, 0, 16, 0, 'IMAGE'], [14, 4, 2, 16, 1, 'VAE'], [15, 16, 0, 3, 3, 'LATENT']], 'groups': [], 'config': {}, 'extra': {}, 'version': 0.4}}}}, 'client_id': 'ad22100d-5eff-4374-b356-771a40bd863b'}
[2024-02-09 19:36] got prompt: {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'red hair', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00002_ (4).png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}
[2024-02-09 19:36] 3
[2024-02-09 19:36] 4
[2024-02-09 19:36] 5
[2024-02-09 19:36] 6
[2024-02-09 19:36] 7
[2024-02-09 19:36] 9
[2024-02-09 19:36] 10
[2024-02-09 19:36] 14
[2024-02-09 19:36] 16
[2024-02-09 19:36] validate (True, None, ['9'], {})
[2024-02-09 19:36] None
[2024-02-09 19:36] /prompt
[2024-02-09 19:36] model_type EPS
[2024-02-09 19:36] adm 2816
[2024-02-09 19:36] Using pytorch attention in VAE
[2024-02-09 19:36] Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[2024-02-09 19:36] Using pytorch attention in VAE
[2024-02-09 19:36] missing {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}
[2024-02-09 19:36] left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])
[2024-02-09 19:36] Requested to load SDXLClipModel
[2024-02-09 19:36] Loading 1 new model
[2024-02-09 19:36] Requested to load AutoencoderKL
[2024-02-09 19:36] Loading 1 new model
[2024-02-09 19:36] Requested to load SDXL
[2024-02-09 19:36] Loading 1 new model
[2024-02-09 19:36] loading in lowvram mode 1731.115385055542
[2024-02-09 19:37] 100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:33<00:00,  1.73s/it]100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:33<00:00,  1.68s/it]
[2024-02-09 19:37] Requested to load AutoencoderKL
[2024-02-09 19:37] Loading 1 new model
[2024-02-09 19:37] Prompt executed in 48.12 seconds
[2024-02-09 19:37] None
[2024-02-09 19:37] /ws
[2024-02-09 19:37] None
[2024-02-09 19:37] /get-output-image
[2024-02-09 19:37] None
[2024-02-09 19:37] /view
[2024-02-09 19:37] None
[2024-02-09 19:37] /upload/image
[2024-02-09 19:37] got prompt
[2024-02-09 19:37] [WARN] ComfyUI-Impact-Pack: Error on prompt - several features will not work.
'inputs'
[2024-02-09 19:37] {'prompt': {'prompt': {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'red hair', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00002_ (5).png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}, 'extra_data': {'extra_pnginfo': {'workflow': {'last_node_id': 16, 'last_link_id': 15, 'nodes': [{'id': 5, 'type': 'EmptyLatentImage', 'pos': [473, 609], 'size': {'0': 315, '1': 106}, 'flags': {}, 'order': 0, 'mode': 0, 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [], 'slot_index': 0}], 'properties': {'Node name for S&R': 'EmptyLatentImage'}, 'widgets_values': [512, 512, 1]}, {'id': 9, 'type': 'SaveImage', 'pos': [1451, 189], 'size': {'0': 210, '1': 270}, 'flags': {}, 'order': 8, 'mode': 0, 'inputs': [{'name': 'images', 'type': 'IMAGE', 'link': 12}], 'properties': {}, 'widgets_values': ['ComfyUI']}, {'id': 14, 'type': 'VAEDecode', 'pos': [1210, 188], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 7, 'mode': 0, 'inputs': [{'name': 'samples', 'type': 'LATENT', 'link': 10}, {'name': 'vae', 'type': 'VAE', 'link': 11}], 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [12], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEDecode'}}, {'id': 10, 'type': 'LoadImage', 'pos': [867, 587], 'size': [315, 313.9999694824219], 'flags': {}, 'order': 1, 'mode': 0, 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [13], 'shape': 3, 'slot_index': 0}, {'name': 'MASK', 'type': 'MASK', 'links': 'null', 'shape': 3}], 'properties': {'Node name for S&R': 'LoadImage'}, 'widgets_values': ['3 idiots.jpg', 'image']}, {'id': 4, 'type': 'CheckpointLoaderSimple', 'pos': [26, 474], 'size': {'0': 315, '1': 98}, 'flags': {}, 'order': 2, 'mode': 0, 'outputs': [{'name': 'MODEL', 'type': 'MODEL', 'links': [1], 'slot_index': 0}, {'name': 'CLIP', 'type': 'CLIP', 'links': [3, 5], 'slot_index': 1}, {'name': 'VAE', 'type': 'VAE', 'links': [11, 14], 'slot_index': 2}], 'properties': {'Node name for S&R': 'CheckpointLoaderSimple'}, 'widgets_values': ['animagineXLV3_v30.safetensors']}, {'id': 16, 'type': 'VAEEncode', 'pos': [1316, 665], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 5, 'mode': 0, 'inputs': [{'name': 'pixels', 'type': 'IMAGE', 'link': 13}, {'name': 'vae', 'type': 'VAE', 'link': 14}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [15], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEEncode'}}, {'id': 3, 'type': 'KSampler', 'pos': [863, 186], 'size': {'0': 315, '1': 262}, 'flags': {}, 'order': 6, 'mode': 0, 'inputs': [{'name': 'model', 'type': 'MODEL', 'link': 1}, {'name': 'positive', 'type': 'CONDITIONING', 'link': 4}, {'name': 'negative', 'type': 'CONDITIONING', 'link': 6}, {'name': 'latent_image', 'type': 'LATENT', 'link': 15}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [10], 'slot_index': 0}], 'properties': {'Node name for S&R': 'KSampler'}, 'widgets_values': [618284639060744, 'randomize', 20, 8, 'euler', 'normal', 1]}, {'id': 6, 'type': 'CLIPTextEncode', 'pos': [415, 186], 'size': {'0': 422.84503173828125, '1': 164.31304931640625}, 'flags': {}, 'order': 3, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 3}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [4], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['change the background to blue and keep the front people the same']}, {'id': 7, 'type': 'CLIPTextEncode', 'pos': [413, 389], 'size': {'0': 425.27801513671875, '1': 180.6060791015625}, 'flags': {}, 'order': 4, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 5}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [6], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['text, watermark,person']}], 'links': [[1, 4, 0, 3, 0, 'MODEL'], [3, 4, 1, 6, 0, 'CLIP'], [4, 6, 0, 3, 1, 'CONDITIONING'], [5, 4, 1, 7, 0, 'CLIP'], [6, 7, 0, 3, 2, 'CONDITIONING'], [10, 3, 0, 14, 0, 'LATENT'], [11, 4, 2, 14, 1, 'VAE'], [12, 14, 0, 9, 0, 'IMAGE'], [13, 10, 0, 16, 0, 'IMAGE'], [14, 4, 2, 16, 1, 'VAE'], [15, 16, 0, 3, 3, 'LATENT']], 'groups': [], 'config': {}, 'extra': {}, 'version': 0.4}}}}, 'client_id': 'ad22100d-5eff-4374-b356-771a40bd863b'}
[2024-02-09 19:37] got prompt: {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'red hair', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00002_ (5).png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}
[2024-02-09 19:37] 3
[2024-02-09 19:37] 4
[2024-02-09 19:37] 5
[2024-02-09 19:37] 6
[2024-02-09 19:37] 7
[2024-02-09 19:37] 9
[2024-02-09 19:37] 10
[2024-02-09 19:37] 14
[2024-02-09 19:37] 16
[2024-02-09 19:37] validate (True, None, ['9'], {})
[2024-02-09 19:37] None
[2024-02-09 19:37] /prompt
[2024-02-09 19:37] 100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:25<00:00,  1.08s/it]100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:25<00:00,  1.30s/it]
[2024-02-09 19:37] Prompt executed in 26.86 seconds
[2024-02-09 19:37] None
[2024-02-09 19:37] /ws
[2024-02-09 19:37] None
[2024-02-09 19:37] /get-output-image
[2024-02-09 19:37] None
[2024-02-09 19:37] /view
[2024-02-09 19:38] None
[2024-02-09 19:38] /get-output-image
[2024-02-09 19:38] None
[2024-02-09 19:38] /view
[2024-02-09 19:38] None
[2024-02-09 19:38] /get-output-image
[2024-02-09 19:38] None
[2024-02-09 19:38] /view
[2024-02-09 19:38] None
[2024-02-09 19:38] /get-output-image
[2024-02-09 19:38] None
[2024-02-09 19:38] /view
[2024-02-09 19:39] None
[2024-02-09 19:39] /upload/image
[2024-02-09 19:39] got prompt
[2024-02-09 19:39] [WARN] ComfyUI-Impact-Pack: Error on prompt - several features will not work.
'inputs'
[2024-02-09 19:39] {'prompt': {'prompt': {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'person red hair', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00002_ (6).png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}, 'extra_data': {'extra_pnginfo': {'workflow': {'last_node_id': 16, 'last_link_id': 15, 'nodes': [{'id': 5, 'type': 'EmptyLatentImage', 'pos': [473, 609], 'size': {'0': 315, '1': 106}, 'flags': {}, 'order': 0, 'mode': 0, 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [], 'slot_index': 0}], 'properties': {'Node name for S&R': 'EmptyLatentImage'}, 'widgets_values': [512, 512, 1]}, {'id': 9, 'type': 'SaveImage', 'pos': [1451, 189], 'size': {'0': 210, '1': 270}, 'flags': {}, 'order': 8, 'mode': 0, 'inputs': [{'name': 'images', 'type': 'IMAGE', 'link': 12}], 'properties': {}, 'widgets_values': ['ComfyUI']}, {'id': 14, 'type': 'VAEDecode', 'pos': [1210, 188], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 7, 'mode': 0, 'inputs': [{'name': 'samples', 'type': 'LATENT', 'link': 10}, {'name': 'vae', 'type': 'VAE', 'link': 11}], 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [12], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEDecode'}}, {'id': 10, 'type': 'LoadImage', 'pos': [867, 587], 'size': [315, 313.9999694824219], 'flags': {}, 'order': 1, 'mode': 0, 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [13], 'shape': 3, 'slot_index': 0}, {'name': 'MASK', 'type': 'MASK', 'links': 'null', 'shape': 3}], 'properties': {'Node name for S&R': 'LoadImage'}, 'widgets_values': ['3 idiots.jpg', 'image']}, {'id': 4, 'type': 'CheckpointLoaderSimple', 'pos': [26, 474], 'size': {'0': 315, '1': 98}, 'flags': {}, 'order': 2, 'mode': 0, 'outputs': [{'name': 'MODEL', 'type': 'MODEL', 'links': [1], 'slot_index': 0}, {'name': 'CLIP', 'type': 'CLIP', 'links': [3, 5], 'slot_index': 1}, {'name': 'VAE', 'type': 'VAE', 'links': [11, 14], 'slot_index': 2}], 'properties': {'Node name for S&R': 'CheckpointLoaderSimple'}, 'widgets_values': ['animagineXLV3_v30.safetensors']}, {'id': 16, 'type': 'VAEEncode', 'pos': [1316, 665], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 5, 'mode': 0, 'inputs': [{'name': 'pixels', 'type': 'IMAGE', 'link': 13}, {'name': 'vae', 'type': 'VAE', 'link': 14}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [15], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEEncode'}}, {'id': 3, 'type': 'KSampler', 'pos': [863, 186], 'size': {'0': 315, '1': 262}, 'flags': {}, 'order': 6, 'mode': 0, 'inputs': [{'name': 'model', 'type': 'MODEL', 'link': 1}, {'name': 'positive', 'type': 'CONDITIONING', 'link': 4}, {'name': 'negative', 'type': 'CONDITIONING', 'link': 6}, {'name': 'latent_image', 'type': 'LATENT', 'link': 15}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [10], 'slot_index': 0}], 'properties': {'Node name for S&R': 'KSampler'}, 'widgets_values': [618284639060744, 'randomize', 20, 8, 'euler', 'normal', 1]}, {'id': 6, 'type': 'CLIPTextEncode', 'pos': [415, 186], 'size': {'0': 422.84503173828125, '1': 164.31304931640625}, 'flags': {}, 'order': 3, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 3}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [4], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['change the background to blue and keep the front people the same']}, {'id': 7, 'type': 'CLIPTextEncode', 'pos': [413, 389], 'size': {'0': 425.27801513671875, '1': 180.6060791015625}, 'flags': {}, 'order': 4, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 5}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [6], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['text, watermark,person']}], 'links': [[1, 4, 0, 3, 0, 'MODEL'], [3, 4, 1, 6, 0, 'CLIP'], [4, 6, 0, 3, 1, 'CONDITIONING'], [5, 4, 1, 7, 0, 'CLIP'], [6, 7, 0, 3, 2, 'CONDITIONING'], [10, 3, 0, 14, 0, 'LATENT'], [11, 4, 2, 14, 1, 'VAE'], [12, 14, 0, 9, 0, 'IMAGE'], [13, 10, 0, 16, 0, 'IMAGE'], [14, 4, 2, 16, 1, 'VAE'], [15, 16, 0, 3, 3, 'LATENT']], 'groups': [], 'config': {}, 'extra': {}, 'version': 0.4}}}}, 'client_id': 'ad22100d-5eff-4374-b356-771a40bd863b'}
[2024-02-09 19:39] got prompt: {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'person red hair', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00002_ (6).png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}
[2024-02-09 19:39] 3
[2024-02-09 19:39] 4
[2024-02-09 19:39] 5
[2024-02-09 19:39] 6
[2024-02-09 19:39] 7
[2024-02-09 19:39] 9
[2024-02-09 19:39] 10
[2024-02-09 19:39] 14
[2024-02-09 19:39] 16
[2024-02-09 19:39] validate (True, None, ['9'], {})
[2024-02-09 19:39] None
[2024-02-09 19:39] /prompt
[2024-02-09 19:39] 100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:40<00:00,  2.11s/it]100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:40<00:00,  2.04s/it]
[2024-02-09 19:39] Prompt executed in 42.49 seconds
[2024-02-09 19:39] None
[2024-02-09 19:39] /ws
[2024-02-09 19:39] None
[2024-02-09 19:39] /get-output-image
[2024-02-09 19:39] None
[2024-02-09 19:39] /view
[2024-02-09 19:39] None
[2024-02-09 19:39] /get-output-image
[2024-02-09 19:39] None
[2024-02-09 19:39] /view
[2024-02-09 19:40] None
[2024-02-09 19:40] /upload/image
[2024-02-09 19:40] got prompt
[2024-02-09 19:40] [WARN] ComfyUI-Impact-Pack: Error on prompt - several features will not work.
'inputs'
[2024-02-09 19:40] {'prompt': {'prompt': {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'change color hair to red', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00004_.png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}, 'extra_data': {'extra_pnginfo': {'workflow': {'last_node_id': 16, 'last_link_id': 15, 'nodes': [{'id': 5, 'type': 'EmptyLatentImage', 'pos': [473, 609], 'size': {'0': 315, '1': 106}, 'flags': {}, 'order': 0, 'mode': 0, 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [], 'slot_index': 0}], 'properties': {'Node name for S&R': 'EmptyLatentImage'}, 'widgets_values': [512, 512, 1]}, {'id': 9, 'type': 'SaveImage', 'pos': [1451, 189], 'size': {'0': 210, '1': 270}, 'flags': {}, 'order': 8, 'mode': 0, 'inputs': [{'name': 'images', 'type': 'IMAGE', 'link': 12}], 'properties': {}, 'widgets_values': ['ComfyUI']}, {'id': 14, 'type': 'VAEDecode', 'pos': [1210, 188], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 7, 'mode': 0, 'inputs': [{'name': 'samples', 'type': 'LATENT', 'link': 10}, {'name': 'vae', 'type': 'VAE', 'link': 11}], 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [12], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEDecode'}}, {'id': 10, 'type': 'LoadImage', 'pos': [867, 587], 'size': [315, 313.9999694824219], 'flags': {}, 'order': 1, 'mode': 0, 'outputs': [{'name': 'IMAGE', 'type': 'IMAGE', 'links': [13], 'shape': 3, 'slot_index': 0}, {'name': 'MASK', 'type': 'MASK', 'links': 'null', 'shape': 3}], 'properties': {'Node name for S&R': 'LoadImage'}, 'widgets_values': ['3 idiots.jpg', 'image']}, {'id': 4, 'type': 'CheckpointLoaderSimple', 'pos': [26, 474], 'size': {'0': 315, '1': 98}, 'flags': {}, 'order': 2, 'mode': 0, 'outputs': [{'name': 'MODEL', 'type': 'MODEL', 'links': [1], 'slot_index': 0}, {'name': 'CLIP', 'type': 'CLIP', 'links': [3, 5], 'slot_index': 1}, {'name': 'VAE', 'type': 'VAE', 'links': [11, 14], 'slot_index': 2}], 'properties': {'Node name for S&R': 'CheckpointLoaderSimple'}, 'widgets_values': ['animagineXLV3_v30.safetensors']}, {'id': 16, 'type': 'VAEEncode', 'pos': [1316, 665], 'size': {'0': 210, '1': 46}, 'flags': {}, 'order': 5, 'mode': 0, 'inputs': [{'name': 'pixels', 'type': 'IMAGE', 'link': 13}, {'name': 'vae', 'type': 'VAE', 'link': 14}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [15], 'shape': 3, 'slot_index': 0}], 'properties': {'Node name for S&R': 'VAEEncode'}}, {'id': 3, 'type': 'KSampler', 'pos': [863, 186], 'size': {'0': 315, '1': 262}, 'flags': {}, 'order': 6, 'mode': 0, 'inputs': [{'name': 'model', 'type': 'MODEL', 'link': 1}, {'name': 'positive', 'type': 'CONDITIONING', 'link': 4}, {'name': 'negative', 'type': 'CONDITIONING', 'link': 6}, {'name': 'latent_image', 'type': 'LATENT', 'link': 15}], 'outputs': [{'name': 'LATENT', 'type': 'LATENT', 'links': [10], 'slot_index': 0}], 'properties': {'Node name for S&R': 'KSampler'}, 'widgets_values': [618284639060744, 'randomize', 20, 8, 'euler', 'normal', 1]}, {'id': 6, 'type': 'CLIPTextEncode', 'pos': [415, 186], 'size': {'0': 422.84503173828125, '1': 164.31304931640625}, 'flags': {}, 'order': 3, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 3}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [4], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['change the background to blue and keep the front people the same']}, {'id': 7, 'type': 'CLIPTextEncode', 'pos': [413, 389], 'size': {'0': 425.27801513671875, '1': 180.6060791015625}, 'flags': {}, 'order': 4, 'mode': 0, 'inputs': [{'name': 'clip', 'type': 'CLIP', 'link': 5}], 'outputs': [{'name': 'CONDITIONING', 'type': 'CONDITIONING', 'links': [6], 'slot_index': 0}], 'properties': {'Node name for S&R': 'CLIPTextEncode'}, 'widgets_values': ['text, watermark,person']}], 'links': [[1, 4, 0, 3, 0, 'MODEL'], [3, 4, 1, 6, 0, 'CLIP'], [4, 6, 0, 3, 1, 'CONDITIONING'], [5, 4, 1, 7, 0, 'CLIP'], [6, 7, 0, 3, 2, 'CONDITIONING'], [10, 3, 0, 14, 0, 'LATENT'], [11, 4, 2, 14, 1, 'VAE'], [12, 14, 0, 9, 0, 'IMAGE'], [13, 10, 0, 16, 0, 'IMAGE'], [14, 4, 2, 16, 1, 'VAE'], [15, 16, 0, 3, 3, 'LATENT']], 'groups': [], 'config': {}, 'extra': {}, 'version': 0.4}}}}, 'client_id': 'ad22100d-5eff-4374-b356-771a40bd863b'}
[2024-02-09 19:40] got prompt: {'3': {'inputs': {'seed': 618284639060744, 'steps': 20, 'cfg': 8, 'sampler_name': 'euler', 'scheduler': 'normal', 'denoise': 1, 'model': ['4', 0], 'positive': ['6', 0], 'negative': ['7', 0], 'latent_image': ['16', 0]}, 'class_type': 'KSampler'}, '4': {'inputs': {'ckpt_name': 'animagineXLV3_v30.safetensors'}, 'class_type': 'CheckpointLoaderSimple'}, '5': {'inputs': {'width': 512, 'height': 512, 'batch_size': 1}, 'class_type': 'EmptyLatentImage'}, '6': {'inputs': {'text': 'change color hair to red', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '7': {'inputs': {'text': 'text, watermark,person', 'clip': ['4', 1]}, 'class_type': 'CLIPTextEncode'}, '9': {'inputs': {'filename_prefix': 'ComfyUI', 'images': ['14', 0]}, 'class_type': 'SaveImage'}, '10': {'inputs': {'image': 'ComfyUI_00004_.png', 'upload': 'image'}, 'class_type': 'LoadImage'}, '14': {'inputs': {'samples': ['3', 0], 'vae': ['4', 2]}, 'class_type': 'VAEDecode'}, '16': {'inputs': {'pixels': ['10', 0], 'vae': ['4', 2]}, 'class_type': 'VAEEncode'}}
[2024-02-09 19:40] 3
[2024-02-09 19:40] 4
[2024-02-09 19:40] 5
[2024-02-09 19:40] 6
[2024-02-09 19:40] 7
[2024-02-09 19:40] 9
[2024-02-09 19:40] 10
[2024-02-09 19:40] 14
[2024-02-09 19:40] 16
[2024-02-09 19:40] validate (True, None, ['9'], {})
[2024-02-09 19:40] None
[2024-02-09 19:40] /prompt
[2024-02-09 19:41] 100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:39<00:00,  1.40s/it]100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:39<00:00,  1.97s/it]
[2024-02-09 19:41] Prompt executed in 41.78 seconds
[2024-02-09 19:41] None
[2024-02-09 19:41] /ws
[2024-02-09 19:41] None
[2024-02-09 19:41] /get-output-image
[2024-02-09 19:41] None
[2024-02-09 19:41] /view
[2024-02-09 19:41] None
[2024-02-09 19:41] /get-output-image
[2024-02-09 19:41] None
[2024-02-09 19:41] /view
